{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on social posts concerning the brand 'Nutella'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camille Strasser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this analysis is to determine the most frequent topics appearing in French-written posts on social media concerning the brand 'Nutella'. Thanks to this analysis, we are able to know with which concepts French-speaking consumers associate Nutella with.\n",
    "\n",
    "The mathematical approach to topic modeling is described in the first section of the analysis. For the business conclusions of the study, you can jump to the second section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Technical approach to topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to discover which topics and keywords are the most frequent in a raw text corpus. The most widely used algorithm for topic modeling, Latent Dirichlet Association (LDA), has been chosen for this quick analysis. We will go step by step through the description of the algorithm:\n",
    "    \n",
    "    - Cleaning the text corpus\n",
    "    - Find a numerical representation\n",
    "    - Perform LDA on the numerical representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Cleaning the raw text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step of the algorithm, the functions in the following package will be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.data_cleaner import DataCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first line of the raw text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0|On est samedi soir il est 22h je suis en pyjama dans mon lit devant la t√©l√© en train de manger des brioches/Nutella et je me les cailles...|√† jakarta le g de nutella il est au m√™me prix que le g de md|Boulettes de fromage, boulettes de viande fourr√©es au fromage et muffin Nutella... L\\'ob√©sit√© morbide me guette üçü|Um pote de Nutella...||@Blandine_Laff @MmmYummyFood juste du Nutella c\\'est parfait üôåüèº|üç´üç´ L\\'arnaque du si√®cle ca....üò°üò° #nutella #gouter #chocolat #chaud #nesquick #tv #montpellier #sud #oklm #bready #pourri #degueulasse #faux #kinder #bueno|\"La Gentilhommi√®re vous invite √† d√©couvrir la nouvelle formule D√©jeuner de F√©vrier\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus_path = '../nutella.csv'\n",
    "raw_corpus =  DataCleaner.get_raw_corpus_from_path(raw_corpus_path)\n",
    "\n",
    "# First line of the corpus\n",
    "example_raw_corpus = raw_corpus[0]\n",
    "example_raw_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from this example to issues in the raw corpus:\n",
    "\n",
    "    - There is internet related content that we won't be able to process using LDA such as emojis\n",
    "    - French is a highly inflected language, i.e. words with the same meaning can appear with different forms (singular/plural, conjugation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 - Dealing with internet related content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some internet related items don't convey any meaning about the content, such as:\n",
    "\n",
    "    -html tags (e.g. <em> some text </em>\n",
    "    -url\n",
    "    -refering to other users (e.g. @username)\n",
    "    \n",
    "Thus they have to be removed from the corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other items, such as emojis and hashtags, convey a meaning related to the topic of a post. So we want to keep them, but they have to be transformed.\n",
    "\n",
    "I chose to:\n",
    "    \n",
    "    - Removing the # in hashtags but keeping the content, for instance #chocolat -> chocolat\n",
    "    - Replacing emojis by their name, for instance üç´ -> chocolate bar\n",
    "    \n",
    "You can note that emojis name are in English, it won't create much trouble in the later part of the algorithm because it is 'language agnostic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can see the results on the first line of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to print a list on the same line\n",
    "def print_on_line(list_words):\n",
    "    line = ''\n",
    "    for word in list_words:\n",
    "        line = line + ' ' + word\n",
    "    return line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 0 On est samedi soir il est 22h je suis en pyjama dans mon lit devant la t√©l√© en train de manger des brioches/Nutella et je me les cailles... √† jakarta le g de nutella il est au m√™me prix que le g de md Boulettes de fromage, boulettes de viande fourr√©es au fromage et muffin Nutella... L\\'ob√©sit√© morbide me guette  french fries  Um pote de Nutella...      juste du Nutella c\\'est parfait  person raising both hands in celebration  emoji modifier fitzpatrick type-3   chocolate bar  chocolate bar  L\\'arnaque du si√®cle ca.... pouting face  pouting face   nutella  gouter  chocolat  chaud  nesquick  tv  montpellier  sud  oklm  bready  pourri  degueulasse  faux  kinder  bueno \"La Gentilhommi√®re vous invite √† d√©couvrir la nouvelle formule D√©jeuner de F√©vrier'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_after_internet_cleaning = DataCleaner('french').get_raw_textual_data_in_document(example_raw_corpus)\n",
    "print_on_line(example_after_internet_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 - Dealing with the language itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some type of words don't convey any topic related meaning, such as:\n",
    "\n",
    "    -punctuation\n",
    "    -pronouns\n",
    "    -adverbs\n",
    "    -preposition ...\n",
    "    \n",
    "They thus need to be removed\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining words, such as verbs and nouns, need to be standardized. This process of standardization is called 'lemmatization'. It consists in finding the form of the word that you could find in a dictionary. For instance, for French verbs, the lemma of a verb is the infinitive form (e.g. veux -> vouloir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results of this cleaning on the first line of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' on samedi soir 22h pyjama lit t√©l√© train manger brioche nutella caille jakarta g nutella m√™me prix g md boulette fromage boulette viande fourrer fromage muffin nutella ob√©sit√© morbide guetter french fries um poter nutella juste nutella parfait person raising both hands in celebration emoji modifier fitzpatrick type-3 chocolate bar chocolate bar arnaque si√®cle ca pouting face pouting face nutella gouter chocolat chaud nesquick tv montpellier sud oklm bready pourrir degueulasse faux kinder bueno gentilhommi√®re inviter d√©couvrir nouveau formule d√©jeuner f√©vrier'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_after_language_cleaning = DataCleaner('french').get_important_lemmas_in_textual_data(example_after_internet_cleaning)\n",
    "print_on_line(example_after_language_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 - Cleaning the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the corpus in so called 'documents' (marked by a number of document | tag). Then we can perform both steps described above to obtain a clean text corpus, split into documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../nutella.csv'\n",
    "clean_text_corpus = DataCleaner('french').get_clean_documents_from_corpus_path(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this process can be long, you save and/or load the clean corpus from/to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "import pickle\n",
    "with open('../clean_text_corpus.pkl', 'wb') as save_file:\n",
    "    save_file = pickle.dump(clean_text_corpus, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "with open('../clean_text_corpus.pkl', 'rb') as load_file:\n",
    "    clean_text_corpus = pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 - A note on implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above mentioned step of language normalization is not perfect since it is based on statistical model. The better the model, the better the normalization. Here, the model are based on not so new algorithms, developed by the Tree Tagger Team in the late 90s. \n",
    "More recent packages such as nltk couldn't be use because they only perform lemmatization on the English language.\n",
    "One way to improve this language normalization step could be to create my own accurate models for the French language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Representing the clean text corpus numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.numerical_corpus import NumericalCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The text corpus need to be represented numerically before being able to process it with the LDA algorithm. To do so, we use a vector space model called the 'Bag of Words' (bow) model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 - Indexing words of the corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we index every word which appears in the corpus. It means that an id is given to each word.\n",
    "For instance, for the first three tweets of the corpus, the word 'nutella' as the id 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' on samedi soir 22h pyjama lit t√©l√© train manger brioche nutella caille jakarta g nutella m√™me prix g md boulette fromage boulette viande fourrer fromage muffin nutella ob√©sit√© morbide guetter french fries'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First three lines as clean text\n",
    "first_tree_tweets = example_after_language_cleaning[:32]\n",
    "print_on_line(first_tree_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'22h': 24,\n",
       " 'boulette': 12,\n",
       " 'brioche': 16,\n",
       " 'caille': 11,\n",
       " 'fourrer': 0,\n",
       " 'french': 7,\n",
       " 'fries': 8,\n",
       " 'fromage': 18,\n",
       " 'g': 19,\n",
       " 'guetter': 9,\n",
       " 'jakarta': 22,\n",
       " 'lit': 25,\n",
       " 'manger': 10,\n",
       " 'md': 20,\n",
       " 'morbide': 26,\n",
       " 'muffin': 21,\n",
       " 'm√™me': 2,\n",
       " 'nutella': 5,\n",
       " 'ob√©sit√©': 1,\n",
       " 'on': 13,\n",
       " 'prix': 17,\n",
       " 'pyjama': 4,\n",
       " 'samedi': 6,\n",
       " 'soir': 3,\n",
       " 'train': 23,\n",
       " 't√©l√©': 15,\n",
       " 'viande': 14}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping\n",
    "mapping = NumericalCorpus.create_mapping_words_to_id_from_text_corpus([first_tree_tweets])\n",
    "mapping.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 - Representing documents by vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document is represented by a vector containing at position k+1 the number of time the word indexed by k appears. For instance, in the first three tweets, the word nutella (indexed by 5) appears three times. Thus the 6th vector coordinate is 3.\n",
    "Such a vector is called a document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 3),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1),\n",
       "  (11, 1),\n",
       "  (12, 2),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 1),\n",
       "  (17, 1),\n",
       "  (18, 2),\n",
       "  (19, 2),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 1),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1)]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, document_vector = NumericalCorpus.build_from_text_corpus([first_tree_tweets])\n",
    "document_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent each document by a vector and arrange them in a matrix, called a document-term matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 - Representing the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent the whole clean text corpus numerically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_word2id, document_term_matrix = NumericalCorpus.build_from_text_corpus(clean_text_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 - Other possible representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is other possible numerical representations, as for instance tf-idf. Bag of words has been chosen because of simplicity, but in a second phase of the project, other numerical representations could be chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 - Applying the LDA algorithm on the numerical corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.topic_modeler import TopicModeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = TopicModeler((dictionary_word2id, document_term_matrix), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.train(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-e196d5feb1c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_most_important_topics_and_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/camille/publicis/topic-modeling-nutella/utils/topic_modeler.py\u001b[0m in \u001b[0;36mget_most_important_topics_and_keywords\u001b[0;34m(self, number_keywords)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_most_important_topics_and_keywords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0munformatted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber_topics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumber_keywords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;31m#formatted_results = TopicModeler.reformat_results(unformatted_results)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munformatted_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/camille/publicis/topic-modeling-nutella/utils/topic_modeler.py\u001b[0m in \u001b[0;36mreformat_results\u001b[0;34m(results)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mreformatted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex_topic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic_results\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mresults_all_keywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTopicModeler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreformat_topics_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0mresult_topic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Topic '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_topic\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': \\n'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresults_all_keywords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mreformatted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreformatted_results\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresult_topic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/camille/publicis/topic-modeling-nutella/utils/topic_modeler.py\u001b[0m in \u001b[0;36mreformat_topics_results\u001b[0;34m(topic_results)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mreformatted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeywords_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mresults_keyword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTopicModeler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreformat_keyword_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m             \u001b[0mreformatted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreformatted_results\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mresults_keyword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mreformatted_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreformatted_results\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/camille/publicis/topic-modeling-nutella/utils/topic_modeler.py\u001b[0m in \u001b[0;36mreformat_keyword_results\u001b[0;34m(keyword)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreformat_keyword_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0msplit_between_weights_and_key_marker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit_between_weights_and_key_marker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mpercentage_weight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mstring_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercentage_weight\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'% \\n'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "model.get_most_important_topics_and_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 - Explanation of the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let A be the document-term matrix from the previous step. Let denote by N the number of documents in the corpus and M the number of words/lemmas in the corpus.\n",
    "Matrix A has as many rows as documents in the corpus, namely N rows, and as many columns as words in the corpus(M).\n",
    "\n",
    "The goal for the LDA algorithm is to uncover:\n",
    "    \n",
    "    - The distribution of topics among the documents\n",
    "    - The distribution of terms among topics\n",
    "    \n",
    "Let denote by B the document-topic matrix. Let denotes by K the number of topics in the corpus. Each row i of matrix B represents the distribution of topics in the document i (for instance 10% of topic 1, 40% of topic 3, 50% of topic 5). B has thus N rows and K columns\n",
    "\n",
    "Let denote by C the topic-term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
