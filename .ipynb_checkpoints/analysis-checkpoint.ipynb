{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_on_line(list_words):\n",
    "    line = ''\n",
    "    for word in list_words:\n",
    "        line = line + ' ' + word\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling on social posts concerning the brand 'Nutella'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camille Strasser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.data_cleaner import DataCleaner\n",
    "from utils.numerical_corpus import NumericalCorpus\n",
    "from utils.topic_modeler import TopicModeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "utils.data_cleaner.DataCleaner"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataCleaner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['on',\n",
       " 'samedi',\n",
       " 'soir',\n",
       " '22h',\n",
       " 'pyjama',\n",
       " 'lit',\n",
       " 'télé',\n",
       " 'train',\n",
       " 'manger',\n",
       " 'brioche',\n",
       " 'nutella',\n",
       " 'caille',\n",
       " 'jakarta',\n",
       " 'g',\n",
       " 'nutella',\n",
       " 'même',\n",
       " 'prix',\n",
       " 'g',\n",
       " 'md',\n",
       " 'boulette',\n",
       " 'fromage',\n",
       " 'boulette',\n",
       " 'viande',\n",
       " 'fourrer',\n",
       " 'fromage',\n",
       " 'muffin',\n",
       " 'nutella',\n",
       " 'obésité',\n",
       " 'morbide',\n",
       " 'guetter',\n",
       " 'french',\n",
       " 'fries',\n",
       " 'um',\n",
       " 'poter',\n",
       " 'nutella',\n",
       " 'juste',\n",
       " 'nutella',\n",
       " 'parfait',\n",
       " 'person',\n",
       " 'raising',\n",
       " 'both',\n",
       " 'hands',\n",
       " 'in',\n",
       " 'celebration',\n",
       " 'emoji',\n",
       " 'modifier',\n",
       " 'fitzpatrick',\n",
       " 'type-3',\n",
       " 'chocolate',\n",
       " 'bar',\n",
       " 'chocolate',\n",
       " 'bar',\n",
       " 'arnaque',\n",
       " 'siècle',\n",
       " 'ca',\n",
       " 'pouting',\n",
       " 'face',\n",
       " 'pouting',\n",
       " 'face',\n",
       " 'nutella',\n",
       " 'gouter',\n",
       " 'chocolat',\n",
       " 'chaud',\n",
       " 'nesquick',\n",
       " 'tv',\n",
       " 'montpellier',\n",
       " 'sud',\n",
       " 'oklm',\n",
       " 'bready',\n",
       " 'pourrir',\n",
       " 'degueulasse',\n",
       " 'faux',\n",
       " 'kinder',\n",
       " 'bueno',\n",
       " 'gentilhommière',\n",
       " 'inviter',\n",
       " 'découvrir',\n",
       " 'nouveau',\n",
       " 'formule',\n",
       " 'déjeuner',\n",
       " 'février',\n",
       " 'menu',\n",
       " 'fevrier',\n",
       " 'salade',\n",
       " 'thaï',\n",
       " 'calamars',\n",
       " 'poires',\n",
       " 'rôties',\n",
       " 'coriandre',\n",
       " 'fraiche',\n",
       " 'wrap',\n",
       " 'jambon',\n",
       " 'd’york',\n",
       " 'légume',\n",
       " 'croquant',\n",
       " 'mayonnaise',\n",
       " 'fort',\n",
       " 'mouclade',\n",
       " 'safranée',\n",
       " 'julienne',\n",
       " 'légumes',\n",
       " 'tarte',\n",
       " 'fin',\n",
       " 'tête',\n",
       " 'veau',\n",
       " 'crème',\n",
       " 'moutarde',\n",
       " 'meaux',\n",
       " 'noix',\n",
       " 'coquilles',\n",
       " 'st',\n",
       " 'jacques',\n",
       " 'juste',\n",
       " 'snackées',\n",
       " '+8.00',\n",
       " '€',\n",
       " 'carottes',\n",
       " 'cumin',\n",
       " 'doux',\n",
       " 'vinaigrette',\n",
       " 'graines',\n",
       " 'grenade',\n",
       " 'blanquette',\n",
       " 'veau',\n",
       " 'tradition',\n",
       " 'riz',\n",
       " 'pilaf',\n",
       " 'cuisse',\n",
       " 'pintade',\n",
       " 'griller',\n",
       " 'peau',\n",
       " 'mousseline',\n",
       " 'chips',\n",
       " 'panais',\n",
       " 'aile',\n",
       " 'raie',\n",
       " 'meunière',\n",
       " 'beurre',\n",
       " 'noisette',\n",
       " 'filet',\n",
       " 'rascasse',\n",
       " 'pistou',\n",
       " 'risotto',\n",
       " 'blé',\n",
       " 'vert',\n",
       " 'duck',\n",
       " 'burger',\n",
       " '+6.00',\n",
       " '€',\n",
       " 'hacher',\n",
       " 'canard',\n",
       " 'pommes',\n",
       " 'terre',\n",
       " 'sautées',\n",
       " 'sur',\n",
       " 'demander',\n",
       " 'assiette',\n",
       " 'fromage',\n",
       " 'de',\n",
       " 'bourgogne',\n",
       " 'd’ailleurs',\n",
       " 'panini',\n",
       " 'au',\n",
       " 'nutella',\n",
       " 'et',\n",
       " 'bananes',\n",
       " 'caramélisées',\n",
       " 'tarte',\n",
       " 'praline',\n",
       " 'rose',\n",
       " 'chantilly',\n",
       " 'vanillé',\n",
       " 'panna',\n",
       " 'cotta',\n",
       " 'sésame',\n",
       " 'noir',\n",
       " 'coulis',\n",
       " 'chocolat',\n",
       " 'nos',\n",
       " 'déjeuners',\n",
       " 'rapides',\n",
       " 'du',\n",
       " 'lundi',\n",
       " 'vendredi',\n",
       " 'même',\n",
       " 'fériés',\n",
       " 'formule',\n",
       " '€',\n",
       " 'entrée',\n",
       " 'plat',\n",
       " 'dessert',\n",
       " 'rêver',\n",
       " 'nutella',\n",
       " 'brioche',\n",
       " 'ouvrir',\n",
       " 'placard',\n",
       " 'dormir',\n",
       " 'dîne',\n",
       " 'photo',\n",
       " 'représentatif',\n",
       " 'folie',\n",
       " 'choisir',\n",
       " 'goûter',\n",
       " 'round',\n",
       " 'pushpin',\n",
       " 'metz',\n",
       " 'smiling',\n",
       " 'face',\n",
       " 'with',\n",
       " 'heart-shaped',\n",
       " 'eyes',\n",
       " 'muffins',\n",
       " 'cookies',\n",
       " 'pâtisserie',\n",
       " 'cupcakes',\n",
       " 'gogo',\n",
       " 'shortcake',\n",
       " 'cookie',\n",
       " 'soft',\n",
       " 'ice',\n",
       " 'cream',\n",
       " 'petit',\n",
       " 'étiquette',\n",
       " 'indiquer',\n",
       " 'daim',\n",
       " 'smarties',\n",
       " 'praliné',\n",
       " 'barbe',\n",
       " 'papa',\n",
       " 'nutella',\n",
       " 'fraise',\n",
       " 'tagada',\n",
       " '..which',\n",
       " 'one',\n",
       " 'would',\n",
       " 'you',\n",
       " 'pick',\n",
       " 'face',\n",
       " 'with',\n",
       " 'stuck-out',\n",
       " 'tongue',\n",
       " 'and',\n",
       " 'winking',\n",
       " 'eye',\n",
       " 'lorrainealacarte',\n",
       " 'metz',\n",
       " 'sweet',\n",
       " 'cupcake',\n",
       " 'cookie',\n",
       " 'muffin',\n",
       " 'cake',\n",
       " 'thursday',\n",
       " 'food',\n",
       " 'foodporn',\n",
       " 'pictureoftheday',\n",
       " 'regram',\n",
       " 'lorrain',\n",
       " 'cute',\n",
       " 'mdr',\n",
       " 'cest',\n",
       " 'bon',\n",
       " 'gauffre',\n",
       " 'chaud',\n",
       " 'nutella',\n",
       " 'couler',\n",
       " 'omgk',\n",
       " 'fdg',\n",
       " '€',\n",
       " 'matin',\n",
       " 'viande',\n",
       " 'semaine',\n",
       " '€',\n",
       " 'soir',\n",
       " 'viande',\n",
       " 'légume',\n",
       " 'frais',\n",
       " 'bébé',\n",
       " 'pain',\n",
       " 'faire',\n",
       " 'petit',\n",
       " 'pot',\n",
       " 'congeler',\n",
       " 'décongeler',\n",
       " 'un',\n",
       " 'semaine',\n",
       " 'temps',\n",
       " 'préparer',\n",
       " 'veille',\n",
       " 'soir',\n",
       " 'légume',\n",
       " 'acheter',\n",
       " 'surgeler',\n",
       " 'carotte',\n",
       " 'brocoli',\n",
       " 'épinard',\n",
       " 'chou',\n",
       " 'fleur',\n",
       " 'boîte',\n",
       " 'haricot',\n",
       " 'vert',\n",
       " 'haricot',\n",
       " 'beurre',\n",
       " 'petit',\n",
       " 'pois',\n",
       " 'flageolet',\n",
       " 'frais',\n",
       " 'courgette',\n",
       " 'poireau',\n",
       " 'carotte',\n",
       " 'champignon',\n",
       " 'pour',\n",
       " 'nutella',\n",
       " 'pot',\n",
       " '700g',\n",
       " 'moi',\n",
       " 'deuz',\n",
       " 'manger',\n",
       " 'mari',\n",
       " 'petit',\n",
       " 'déj',\n",
       " 'grand',\n",
       " 'allergique',\n",
       " 'aimer',\n",
       " 'awwwwwwwwwwww',\n",
       " 'pub',\n",
       " 'nutella',\n",
       " 'game',\n",
       " 'boy',\n",
       " '<',\n",
       " 'genre',\n",
       " 'mec',\n",
       " 'ruiner',\n",
       " 'an',\n",
       " 'effort',\n",
       " 'manger',\n",
       " 'baguette',\n",
       " 'beurre',\n",
       " 'et',\n",
       " 'nutella',\n",
       " 'mourirgros',\n",
       " 'crêpe',\n",
       " 'nutella',\n",
       " 'dire',\n",
       " 'faire',\n",
       " 'sport',\n",
       " 'semaine',\n",
       " 'monfils',\n",
       " 'tsonga',\n",
       " 'nadal',\n",
       " 'diara',\n",
       " 'blessé',\n",
       " 'venir',\n",
       " 'manger',\n",
       " 'nutella',\n",
       " 'bébé',\n",
       " 'faire',\n",
       " 'chocolat',\n",
       " 'chaud',\n",
       " 'tartine',\n",
       " 'nutella❤',\n",
       " 'nutella',\n",
       " 'france',\n",
       " 'officiel',\n",
       " 'aimer',\n",
       " 'twitt',\n",
       " 'propos',\n",
       " 'chandeleur',\n",
       " 'relieved',\n",
       " 'face']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_corpus_2[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_2 = open('unit_tests/example_data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_2.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = open('../nutella.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this analysis is to determine the most frequent topics appearing in French-written posts on social media concerning the brand 'Nutella'. Thanks to this analysis, we are able to know with which concepts French-speaking consumers associate Nutella with.\n",
    "\n",
    "The mathematical approach to topic modeling is described in the first section of the analysis. For the business conclusions of the study, you can jump to the second section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Technical approach to topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want to discover which topics and keywords are the most frequent in a raw text corpus. The most widely used algorithm for topic modeling, Latent Dirichlet Association (LDA), has been chosen for this quick analysis. We will go step by step through the description of the algorithm:\n",
    "    \n",
    "    - Cleaning the text corpus\n",
    "    - Find a numerical representation\n",
    "    - Perform LDA on the numerical representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 - Cleaning the raw text corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this step of the algorithm, the functions in the following package will be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils.data_cleaner import DataCleaner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the first line of the raw text corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0|On est samedi soir il est 22h je suis en pyjama dans mon lit devant la télé en train de manger des brioches/Nutella et je me les cailles...|à jakarta le g de nutella il est au même prix que le g de md|Boulettes de fromage, boulettes de viande fourrées au fromage et muffin Nutella... L\\'obésité morbide me guette 🍟|Um pote de Nutella...||@Blandine_Laff @MmmYummyFood juste du Nutella c\\'est parfait 🙌🏼|🍫🍫 L\\'arnaque du siècle ca....😡😡 #nutella #gouter #chocolat #chaud #nesquick #tv #montpellier #sud #oklm #bready #pourri #degueulasse #faux #kinder #bueno|\"La Gentilhommière vous invite à découvrir la nouvelle formule Déjeuner de Février\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus_path = '../nutella.csv'\n",
    "raw_corpus =  DataCleaner.get_raw_corpus_from_path(raw_corpus_path)\n",
    "\n",
    "# First line of the corpus\n",
    "example_raw_corpus = raw_corpus[0]\n",
    "example_raw_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe from this example to issues in the raw corpus:\n",
    "\n",
    "    - There is internet related content that we won't be able to process using LDA such as emojis\n",
    "    - French is a highly inflected language, i.e. words with the same meaning can appear with different forms (singular/plural, conjugation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 - Dealing with internet related content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some internet related items don't convey any meaning about the content, such as:\n",
    "\n",
    "    -html tags (e.g. <em> some text </em>\n",
    "    -url\n",
    "    -refering to other users (e.g. @username)\n",
    "    \n",
    "Thus they have to be removed from the corpus\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some other items, such as emojis and hashtags, convey a meaning related to the topic of a post. So we want to keep them, but they have to be transformed.\n",
    "\n",
    "I chose to:\n",
    "    \n",
    "    - Removing the # in hashtags but keeping the content, for instance #chocolat -> chocolat\n",
    "    - Replacing emojis by their name, for instance 🍫 -> chocolate bar\n",
    "    \n",
    "You can note that emojis name are in English, it won't create much trouble in the later part of the algorithm because it is 'language agnostic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we can see the results on the first line of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 0 On est samedi soir il est 22h je suis en pyjama dans mon lit devant la télé en train de manger des brioches/Nutella et je me les cailles... à jakarta le g de nutella il est au même prix que le g de md Boulettes de fromage, boulettes de viande fourrées au fromage et muffin Nutella... L\\'obésité morbide me guette  french fries  Um pote de Nutella...      juste du Nutella c\\'est parfait  person raising both hands in celebration  emoji modifier fitzpatrick type-3   chocolate bar  chocolate bar  L\\'arnaque du siècle ca.... pouting face  pouting face   nutella  gouter  chocolat  chaud  nesquick  tv  montpellier  sud  oklm  bready  pourri  degueulasse  faux  kinder  bueno \"La Gentilhommière vous invite à découvrir la nouvelle formule Déjeuner de Février'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_after_internet_cleaning = DataCleaner('french').get_raw_textual_data_in_document(example_raw_corpus)\n",
    "print_on_line(example_after_internet_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 - Dealing with the language itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some type of words don't convey any topic related meaning, such as:\n",
    "\n",
    "    -punctuation\n",
    "    -pronouns\n",
    "    -adverbs\n",
    "    -preposition ...\n",
    "    \n",
    "They thus need to be removed\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remaining words, such as verbs and nouns, need to be standardized. This process of standardization is called 'lemmatization'. It consists in finding the form of the word that you could find in a dictionary. For instance, for French verbs, the lemma of a verb is the infinitive form (e.g. veux -> vouloir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the results of this cleaning on the first line of the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' on samedi soir 22h pyjama lit télé train manger brioche nutella caille jakarta g nutella même prix g md boulette fromage boulette viande fourrer fromage muffin nutella obésité morbide guetter french fries um poter nutella juste nutella parfait person raising both hands in celebration emoji modifier fitzpatrick type-3 chocolate bar chocolate bar arnaque siècle ca pouting face pouting face nutella gouter chocolat chaud nesquick tv montpellier sud oklm bready pourrir degueulasse faux kinder bueno gentilhommière inviter découvrir nouveau formule déjeuner février'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_after_language_cleaning = DataCleaner('french').get_important_lemmas_in_textual_data(example_after_internet_cleaning)\n",
    "print_on_line(example_after_language_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 - Cleaning the whole corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first split the corpus in so called 'documents' (marked by a number of document | tag). Then we can perform both steps described above to obtain a clean text corpus, split into documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../nutella.csv'\n",
    "text_corpus = DataCleaner('french').get_clean_documents_from_corpus_path(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this process can be long, you save and/or load the clean corpus from/to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save\n",
    "with open('../clean_text_corpus.pkl', 'wb') as save_file:\n",
    "    save_file = pickle.dump(text_corpus, save_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load\n",
    "with open('../clean_text_corpus.pkl', 'rb') as load_file:\n",
    "    clean_text_corpus = pickle.load(load_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 - A note on implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above mentioned step of language normalization is not perfect since it is based on statistical model. The better the model, the better the normalization. Here, the model are based on not so new algorithms, developed by the Tree Tagger Team in the late 90s. \n",
    "More recent packages such as nltk couldn't be use because they only perform lemmatization on the English language.\n",
    "One way to improve this language normalization step could be to create my own accurate models for the French language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
